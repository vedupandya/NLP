{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3eHsXi/c9bU1B0uisAm7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedupandya/NLP/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "38K-fl2_ZfPt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"cnn_dailymail\",'1.0.0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nIIbnR4ZtbL",
        "outputId": "2380c65e-d6e4-4ec6-9ec7-d6bf5ba54d90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset['train']['article'][:100]"
      ],
      "metadata": {
        "id": "KXs5BUX_crfZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "import spacy"
      ],
      "metadata": {
        "id": "E3uGlM0ma6py"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
      ],
      "metadata": {
        "id": "64DL46P6qeFZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_phrases = []\n",
        "for i in range(100):\n",
        "    text = data[i]\n",
        "    doc = nlp(text)\n",
        "    for chunk in doc.noun_chunks:\n",
        "        filtered_phrase = ' '.join(token.text for token in chunk if token.is_alpha and token.text.lower() not in stopwords)\n",
        "        if filtered_phrase:\n",
        "            noun_phrases.append(filtered_phrase.lower())"
      ],
      "metadata": {
        "id": "rezCBpQXqscL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "fzFGNwDq2v9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_phrases = [phrase.split() for phrase in noun_phrases]"
      ],
      "metadata": {
        "id": "mD2kykKU8CFg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "model = gensim.models.Word2Vec(tokenized_phrases, min_count=1, vector_size=100,window=5,workers=-1)"
      ],
      "metadata": {
        "id": "6brl6qxUa5K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top - k"
      ],
      "metadata": {
        "id": "4IxZPjYh21e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "all_words = noun_phrases\n",
        "word_counts = Counter(all_words)\n",
        "top_phrases = word_counts.most_common(k)\n",
        "\n",
        "print(f\"Top {k} most frequent phrases:\")\n",
        "for phrase, count in top_phrases:\n",
        "    print(f\"{phrase}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FITboVh_ck3L",
        "outputId": "18a122a2-920c-4091-985e-39e11c55c698"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent phrases:\n",
            "friend: 106\n",
            "cnn: 105\n",
            "mail: 101\n",
            "e: 99\n",
            "people: 97\n",
            "iraq: 61\n",
            "time: 49\n",
            "children: 43\n",
            "police: 40\n",
            "president: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [top_phrases[i][0] for i in range(k)]\n",
        "for i in words:\n",
        "  similar_words = model.wv.most_similar(i, topn=10)\n",
        "  print(f\"Words similar to '{i}'\")\n",
        "  for word, similarity in similar_words:\n",
        "    print(word)\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOwc-MXzyINb",
        "outputId": "812618e1-0484-4090-a9af-0fd3cfdc59ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'friend'\n",
            "registration\n",
            "law\n",
            "benz\n",
            "gunshot\n",
            "hero\n",
            "hashimi\n",
            "brake\n",
            "clinic\n",
            "n\n",
            "gel\n",
            "--------------------------------------------------\n",
            "Words similar to 'cnn'\n",
            "days\n",
            "investment\n",
            "pvt\n",
            "osasuna\n",
            "kivu\n",
            "token\n",
            "raphael\n",
            "follow\n",
            "press\n",
            "necessities\n",
            "--------------------------------------------------\n",
            "Words similar to 'mail'\n",
            "cancellation\n",
            "analysis\n",
            "offers\n",
            "elton\n",
            "dispute\n",
            "allies\n",
            "hurricanes\n",
            "district\n",
            "ecology\n",
            "raspberry\n",
            "--------------------------------------------------\n",
            "Words similar to 'e'\n",
            "seizures\n",
            "algiers\n",
            "borussia\n",
            "knocked\n",
            "whacked\n",
            "threats\n",
            "tissues\n",
            "brazilians\n",
            "raleigh\n",
            "oncologist\n",
            "--------------------------------------------------\n",
            "Words similar to 'people'\n",
            "supporter\n",
            "carmindy\n",
            "kh\n",
            "bookstores\n",
            "smoking\n",
            "permanent\n",
            "roma\n",
            "upcoming\n",
            "glass\n",
            "haas\n",
            "--------------------------------------------------\n",
            "Words similar to 'iraq'\n",
            "department\n",
            "waco\n",
            "diffusion\n",
            "professional\n",
            "illegal\n",
            "officials\n",
            "ill\n",
            "turns\n",
            "ipcc\n",
            "wonderful\n",
            "--------------------------------------------------\n",
            "Words similar to 'time'\n",
            "alternative\n",
            "rescue\n",
            "tennessee\n",
            "blackface\n",
            "partner\n",
            "oil\n",
            "walsh\n",
            "old\n",
            "fighting\n",
            "rush\n",
            "--------------------------------------------------\n",
            "Words similar to 'children'\n",
            "string\n",
            "presidency\n",
            "fire\n",
            "change\n",
            "guns\n",
            "ideals\n",
            "songkhla\n",
            "columbia\n",
            "katie\n",
            "centres\n",
            "--------------------------------------------------\n",
            "Words similar to 'police'\n",
            "computer\n",
            "mode\n",
            "pretenses\n",
            "topic\n",
            "aknoun\n",
            "woman\n",
            "stop\n",
            "tide\n",
            "geneva\n",
            "ankle\n",
            "--------------------------------------------------\n",
            "Words similar to 'president'\n",
            "description\n",
            "icon\n",
            "desk\n",
            "tuesday\n",
            "miller\n",
            "reuter\n",
            "members\n",
            "australian\n",
            "innings\n",
            "gandhi\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra"
      ],
      "metadata": {
        "id": "xYfUZsGh_Y_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the phrase vectors\n",
        "def get_phrase_vector(phrase):\n",
        "    phrase_vector = np.zeros(model.vector_size)\n",
        "    for word in phrase:\n",
        "        if word in model.wv:\n",
        "            phrase_vector += model.wv[word]\n",
        "    return phrase_vector"
      ],
      "metadata": {
        "id": "p1--p6TObqwa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pvec = [get_phrase_vector(p.split()) for p in noun_phrases]"
      ],
      "metadata": {
        "id": "maTqzNQ2zt6u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_phrases[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Nr9W5VKl_Shp",
        "outputId": "8d9b5d43-3156-4d08-eca5-f22b46de9d5f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reported million million fortune'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.wv.similar_by_vector(pvec[5], topn=5)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK1z9BGO2Abh",
        "outputId": "127a3f76-25bf-442a-f26b-06742d5dd5ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('million', 0.8218741416931152),\n",
              " ('reported', 0.41035813093185425),\n",
              " ('fortune', 0.32655957341194153),\n",
              " ('massa', 0.32347962260246277),\n",
              " ('dress', 0.31504026055336)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}